{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3d24c170",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use mps:0\n",
      "Device set to use mps:0\n",
      "Device set to use mps:0\n",
      "Device set to use mps:0\n",
      "Device set to use mps:0\n",
      "Device set to use mps:0\n",
      "Device set to use mps:0\n",
      "Device set to use mps:0\n",
      "Device set to use mps:0\n",
      "Device set to use mps:0\n",
      "Device set to use mps:0\n",
      "Device set to use mps:0\n",
      "Device set to use mps:0\n",
      "Device set to use mps:0\n",
      "Device set to use mps:0\n",
      "Device set to use mps:0\n",
      "Device set to use mps:0\n",
      "Device set to use mps:0\n",
      "Device set to use mps:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                            Question  \\\n",
      "0  What is the most obvious problem people bring ...   \n",
      "1  Are there any recurring themes in the feedback...   \n",
      "2  What are the common suggestions for improving ...   \n",
      "3  Is there any feedback regarding the pricing of...   \n",
      "4  What aspects of our service do customers appre...   \n",
      "5  Are there any complaints about the new feature...   \n",
      "6  What is the general sentiment about our suppor...   \n",
      "7  Do customers mention any specific competitors ...   \n",
      "8  Are there any suggestions for new features or ...   \n",
      "9  What are the main pain points for users in the...   \n",
      "\n",
      "                                              Answer  \n",
      "0  Meetings often extend beyond the scheduled tim...  \n",
      "1                                   release analysis  \n",
      "2  calendar slots would improve engineering outpu...  \n",
      "3  Meetings often extend beyond the scheduled tim...  \n",
      "4  Meetings often extend beyond the scheduled tim...  \n",
      "5                                    attention spans  \n",
      "6                                   release analysis  \n",
      "7                                           calendar  \n",
      "8                                         exhausting  \n",
      "9                                   release analysis  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from transformers import pipeline\n",
    "\n",
    "def answer_question_from_csv(question: str, csv_file: str) -> str:\n",
    "    \"\"\"\n",
    "    Answers a question based on feedback data from a CSV file.\n",
    "\n",
    "    Parameters:\n",
    "    - question (str): The question to be answered.\n",
    "    - csv_file (str): Path to the CSV file containing feedback data.\n",
    "\n",
    "    Returns:\n",
    "    - str: The answer extracted from the feedback data.\n",
    "    \"\"\"\n",
    "    # Load the feedback data\n",
    "    try:\n",
    "        df = pd.read_csv(csv_file)\n",
    "    except FileNotFoundError:\n",
    "        return f\"Error: The file '{csv_file}' was not found.\"\n",
    "    except pd.errors.EmptyDataError:\n",
    "        return f\"Error: The file '{csv_file}' is empty.\"\n",
    "    except pd.errors.ParserError:\n",
    "        return f\"Error: The file '{csv_file}' could not be parsed.\"\n",
    "\n",
    "    # Check if the 'Feedback' column exists\n",
    "    if 'Feedback' not in df.columns:\n",
    "        return \"Error: The CSV file does not contain a 'Feedback' column.\"\n",
    "\n",
    "    # Combine all feedback entries into a single context\n",
    "    context = \"\\n\".join(df['Feedback'].dropna().astype(str).tolist())\n",
    "\n",
    "    # Define the system prompt\n",
    "    system_prompt = (\"System: You are an AI assistant that provides anonymous transformations of user feedback. \"\n",
    "                     \"Never identify an individual user. Based on the following feedback, answer the question. Question: \")\n",
    "    \n",
    "    # Prepend the system prompt to the user's question\n",
    "    modified_question = f\"{system_prompt}{question}\"\n",
    "\n",
    "    # Initialize the question-answering pipeline with a pre-trained model\n",
    "    qa_pipeline = pipeline(\"question-answering\", model=\"distilbert-base-uncased-distilled-squad\")\n",
    "\n",
    "    # Use the pipeline to answer the question based on the context\n",
    "    result = qa_pipeline(question=modified_question, context=context)\n",
    "\n",
    "    return result['answer']\n",
    "\n",
    "\n",
    "# List to store questions and answers\n",
    "results_list = []\n",
    "\n",
    "question1 = \"What is the most obvious problem people bring up across all departments\"\n",
    "answer1 = answer_question_from_csv(question1, \"feedback.csv\")\n",
    "results_list.append({'Question': question1, 'Answer': answer1})\n",
    "\n",
    "question2 = \"Are there any recurring themes in the feedback related to product usability?\"\n",
    "answer2 = answer_question_from_csv(question2, \"feedback.csv\")\n",
    "results_list.append({'Question': question2, 'Answer': answer2})\n",
    "\n",
    "question3 = \"What are the common suggestions for improving customer service?\"\n",
    "answer3 = answer_question_from_csv(question3, \"feedback.csv\")\n",
    "results_list.append({'Question': question3, 'Answer': answer3})\n",
    "\n",
    "question4 = \"Is there any feedback regarding the pricing of our products or services?\"\n",
    "answer4 = answer_question_from_csv(question4, \"feedback.csv\")\n",
    "results_list.append({'Question': question4, 'Answer': answer4})\n",
    "\n",
    "question5 = \"What aspects of our service do customers appreciate the most?\"\n",
    "answer5 = answer_question_from_csv(question5, \"feedback.csv\")\n",
    "results_list.append({'Question': question5, 'Answer': answer5})\n",
    "\n",
    "question6 = \"Are there any complaints about the new feature released last quarter?\"\n",
    "answer6 = answer_question_from_csv(question6, \"feedback.csv\")\n",
    "results_list.append({'Question': question6, 'Answer': answer6})\n",
    "\n",
    "question7 = \"What is the general sentiment about our support team's responsiveness?\"\n",
    "answer7 = answer_question_from_csv(question7, \"feedback.csv\")\n",
    "results_list.append({'Question': question7, 'Answer': answer7})\n",
    "\n",
    "question8 = \"Do customers mention any specific competitors in their feedback?\"\n",
    "answer8 = answer_question_from_csv(question8, \"feedback.csv\")\n",
    "results_list.append({'Question': question8, 'Answer': answer8})\n",
    "\n",
    "question9 = \"Are there any suggestions for new features or services?\"\n",
    "answer9 = answer_question_from_csv(question9, \"feedback.csv\")\n",
    "results_list.append({'Question': question9, 'Answer': answer9})\n",
    "\n",
    "question10 = \"What are the main pain points for users in the onboarding process?\"\n",
    "answer10 = answer_question_from_csv(question10, \"feedback.csv\")\n",
    "results_list.append({'Question': question10, 'Answer': answer10})\n",
    "\n",
    "# Convert the list of results to a Pandas DataFrame\n",
    "results_df = pd.DataFrame(results_list)\n",
    "\n",
    "# Print the DataFrame\n",
    "print(results_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52f16d0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting topics using LLM...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use mps:0\n",
      "Both `max_new_tokens` (=256) and `max_length`(=150) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=256) and `max_length`(=150) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from transformers import pipeline\n",
    "import matplotlib.pyplot as plt\n",
    "import json # Used conceptually for the topic list format\n",
    "\n",
    "# Suppress Hugging Face pipeline logging for cleaner output, if desired\n",
    "import logging\n",
    "logging.getLogger(\"transformers.pipeline\").setLevel(logging.ERROR)\n",
    "\n",
    "def perform_topic_modeling():\n",
    "    # 1. Load feedback data\n",
    "    try:\n",
    "        df_feedback = pd.read_csv(\"feedback.csv\")\n",
    "        if 'Feedback' not in df_feedback.columns:\n",
    "            print(\"Error: 'Feedback' column not found in feedback.csv\")\n",
    "            return {}\n",
    "        feedback_list = df_feedback['Feedback'].dropna().astype(str).tolist()\n",
    "        if not feedback_list:\n",
    "            print(\"No feedback data to process.\")\n",
    "            return {}\n",
    "        full_context = \"\\n\".join(feedback_list)\n",
    "    except FileNotFoundError:\n",
    "        print(\"Error: feedback.csv not found.\")\n",
    "        return {}\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading feedback data: {e}\")\n",
    "        return {}\n",
    "\n",
    "    # 2. Topic Extraction by LLM\n",
    "    print(\"Extracting topics using LLM...\")\n",
    "    topic_counts = {} # Initialize topic_counts here to ensure it's always defined\n",
    "    try:\n",
    "        # Using a text2text-generation model to extract topics\n",
    "        # Using a smaller model for speed. Larger models (e.g., flan-t5-base) might yield better topics.\n",
    "        topic_extractor = pipeline(\"text2text-generation\", model=\"google/flan-t5-small\")\n",
    "        \n",
    "        # Truncate context if too long for the model (Flan-T5 typical limit is 512 tokens)\n",
    "        # This is a rough character-based truncation; token-based would be more precise.\n",
    "        max_input_chars = 2000 # Approx. 500 tokens\n",
    "        truncated_context = full_context[:max_input_chars]\n",
    "        \n",
    "        prompt = f\"Based on the following user feedback, identify and list exactly 7 main topics. Ensure diversity in the topics. Output the topics as a comma-separated list. Feedback: {truncated_context}\"\n",
    "        # Increase max_length if topics are long or many\n",
    "        extracted_topics_text = topic_extractor(prompt, max_length=150, num_beams=3)[0]['generated_text']\n",
    "        \n",
    "        topics = [topic.strip() for topic in extracted_topics_text.split(',') if topic.strip()]\n",
    "        \n",
    "        if not topics:\n",
    "            print(\"LLM could not extract any topics. Topic modeling cannot proceed without topics.\")\n",
    "            return {} # Exit if no topics are extracted\n",
    "        else:\n",
    "            print(f\"Extracted Topics: {topics}\")\n",
    "        # The 'topics' list is conceptually what might be saved to a JSON file.\n",
    "        # print(f\"Topics as JSON: {json.dumps(topics)}\") \n",
    "    except Exception as e:\n",
    "        print(f\"Error during topic extraction: {e}. Topic modeling cannot proceed.\")\n",
    "        return {} # Exit if there's an error during extraction\n",
    "\n",
    "    # 3. Count feedback on each topic\n",
    "    print(\"\\nCounting feedback per topic...\")\n",
    "    try:\n",
    "        # Using zero-shot-classification for assigning feedback to topics\n",
    "        classifier = pipeline(\"zero-shot-classification\", model=\"facebook/bart-large-mnli\")\n",
    "        topic_counts = {topic: 0 for topic in topics} # Re-initialize based on extracted topics\n",
    "        \n",
    "        for i, feedback_item in enumerate(feedback_list):\n",
    "            if not feedback_item.strip():\n",
    "                continue\n",
    "            # Basic progress indicator\n",
    "            if (i + 1) % 10 == 0 or i == len(feedback_list) -1:\n",
    "                 print(f\"  Processing feedback item {i+1}/{len(feedback_list)}\")\n",
    "            try:\n",
    "                # Models have token limits, truncate if necessary\n",
    "                # BART's typical limit is 1024 tokens, but pipeline might handle it. Be safe.\n",
    "                max_feedback_chars = 1500 # Approx 300-400 tokens\n",
    "                truncated_feedback_item = feedback_item[:max_feedback_chars]\n",
    "                \n",
    "                # multi_label=False assumes one primary topic per feedback item\n",
    "                classification_result = classifier(truncated_feedback_item, candidate_labels=topics, multi_label=False)\n",
    "                \n",
    "                if classification_result['scores'] and classification_result['labels']:\n",
    "                    best_topic = classification_result['labels'][0] # Topic with the highest score\n",
    "                    topic_counts[best_topic] += 1\n",
    "            except Exception as item_e:\n",
    "                print(f\"    Skipping a feedback item due to error: {item_e}\")\n",
    "                continue # Skip to next feedback item if one fails\n",
    "        \n",
    "        print(f\"\\nTopic Counts: {json.dumps(topic_counts, indent=2)}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error during feedback classification: {e}\")\n",
    "        return topic_counts # Return current counts even if classification fails mid-way\n",
    "\n",
    "    # 4. Plot histogram\n",
    "    if any(topic_counts.values()):\n",
    "        print(\"\\nPlotting topic distribution...\")\n",
    "        topic_names = list(topic_counts.keys())\n",
    "        counts = list(topic_counts.values())\n",
    "\n",
    "        plt.figure(figsize=(12, 7))\n",
    "        bars = plt.bar(topic_names, counts, color='skyblue')\n",
    "        plt.xlabel(\"Topics\", fontsize=12)\n",
    "        plt.ylabel(\"Number of Feedback Items\", fontsize=12)\n",
    "        plt.title(\"Feedback Distribution by Extracted Topic\", fontsize=14)\n",
    "        plt.xticks(rotation=45, ha=\"right\", fontsize=10)\n",
    "        plt.yticks(fontsize=10)\n",
    "        plt.grid(axis='y', linestyle='--')\n",
    "        plt.tight_layout() # Adjust layout to prevent labels from overlapping\n",
    "        # Add counts on top of bars\n",
    "        for bar in bars:\n",
    "            yval = bar.get_height()\n",
    "            plt.text(bar.get_x() + bar.get_width()/2.0, yval + 0.05 * max(counts), int(yval), ha='center', va='bottom', fontsize=9)\n",
    "        plt.show()\n",
    "    else:\n",
    "        print(\"No topic counts to plot (all counts might be zero or topics list was empty).\")\n",
    "    \n",
    "    return topic_counts\n",
    "\n",
    "# Run the topic modeling process\n",
    "histogram_values = perform_topic_modeling()\n",
    "print(\"\\nReturned Histogram Values (Topic Counts):\")\n",
    "print(json.dumps(histogram_values, indent=2))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
